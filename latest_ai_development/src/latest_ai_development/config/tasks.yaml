# Phase 1 MVP Tasks for Deep Research Crew

discovery_task:
  description: >
    Analyze the research query: "{query}"
    
    Break down the query into specific research questions, identify key concepts,
    and determine the most relevant sources to investigate. Create a research plan
    that outlines which documents to prioritize and what claims to look for.
    
    Consider the date range ({start_date} to {end_date}) and source types: {sources}
  expected_output: >
    A structured research plan with:
    - 3-5 specific research questions derived from the query
    - List of prioritized source types and search strategies
    - Key concepts and entities to track
    - Success criteria for the research
  agent: research_lead

literature_mining_task:
  description: >
    Based on the research plan, find and process relevant documents about "{query}".
    
    Use the PDF parser and web scraper tools to extract structured information from
    up to {max_docs} documents. For each document, extract:
    - Title, authors, and metadata
    - Abstract and key sections (methods, results, conclusions)
    - Citations and references
    - Datasets and code repositories mentioned
    
    Focus on high-quality sources published between {start_date} and {end_date}.
  expected_output: >
    A collection of structured documents with:
    - Full metadata and provenance
    - Extracted sections and content
    - Citations and references
    - Identified datasets and code links
    Format as JSON list of Document objects
  agent: literature_miner

claim_extraction_task:
  description: >
    Extract factual claims from the processed documents.
    
    Analyze each document's content and identify:
    - Numerical claims (performance metrics, percentages, measurements)
    - Comparative claims (better than, faster than, etc.)
    - Experimental results and findings
    - Theoretical assertions
    
    For each claim, capture:
    - The exact claim text with context
    - Claim type and entities involved
    - Provenance (source, page, section)
    - Any metrics or numerical values
  expected_output: >
    A list of extracted claims with full provenance:
    - Claim ID and text
    - Claim type (numerical, comparative, experimental, theoretical)
    - Source document and location
    - Entities and metrics
    - Context from surrounding text
    Format as JSON list of Claim objects
  agent: literature_miner

fact_checking_task:
  description: >
    Verify the extracted claims against external sources.
    
    For each claim:
    1. Generate appropriate search queries
    2. Search academic databases (Semantic Scholar, etc.)
    3. Find supporting or contradicting evidence
    4. Assign confidence levels based on evidence quality and quantity
    
    Confidence levels:
    - VERIFIED: Strong supporting evidence from multiple sources
    - PARTIALLY_VERIFIED: Some supporting evidence, but limited
    - CONTRADICTED: Evidence contradicts the claim
    - UNKNOWN: Insufficient evidence to verify
  expected_output: >
    Updated claims with verification results:
    - Confidence level for each claim
    - List of supporting/contradicting evidence
    - Evidence sources and relevance scores
    - Verification notes and methodology
    Format as JSON list of verified Claim objects
  agent: claim_verifier

report_generation_task:
  description: >
    Create a comprehensive research report synthesizing all findings.
    
    The report should include:
    1. Executive Summary (2-3 paragraphs)
       - Overview of research question
       - Key findings and conclusions
       - Confidence in findings
    
    2. Research Statistics
       - Number of documents processed
       - Number of claims extracted and verified
       - Breakdown by confidence level
    
    3. Claims Analysis
       - Verified claims with evidence
       - Partially verified claims
       - Contradicted claims
       - Unverified claims
    
    4. Source Documents
       - List of all sources with metadata
       - Links and citations
    
    5. Provenance and Reproducibility
       - How claims were extracted and verified
       - Links to original sources
    
    Use clear, professional language. Include confidence indicators and evidence
    for all major claims. Make the report accessible to both technical and
    non-technical audiences.
  expected_output: >
    A complete research report in Markdown format with:
    - Executive summary and key findings
    - Detailed claims analysis grouped by confidence
    - Full source documentation
    - Provenance information for reproducibility
    Save to file: research_report.md
  agent: report_composer
  output_file: research_report.md
